{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# helper modules\n",
    "from ml_utils import pickle_classifier, load_classifier, ExtractFeature, \\\n",
    "                     precision_recall_matrix, get_classifier_results\n",
    "from nlp_helper import CleanTextTransformer, tokenize_text\n",
    "from query_events import execute_query\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logname = 'log/event_classifier_log'\n",
    "logging.basicConfig(filename=logname,\n",
    "                    filemode='a',\n",
    "                    format='%(asctime)s -  %(name)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='This is the event classifier program.')\n",
    "parser.add_argument('--level', help='Level of classification, \\\n",
    "        usage: --level primary', choices=[\"primary\", \"secondary\", \"tertiary\"], required=True)\n",
    "parser.add_argument('--retrain', help='Retrain classifier. usage: --retrain F',\n",
    "                    choices=['T', 'F'], required=True)\n",
    "parser.add_argument('--load_clf', help='Load existing classifier. \\\n",
    "        usage: --load_clf classifiers/SVM_06202017121413.pkl', nargs='+', required=False)\n",
    "parser.add_argument('--event_ids', help='Enter events to classify. If blank \\\n",
    "        then query will fetch all training data. usage: --event_ids 998746 \\\n",
    "        33384956 114992', nargs='+', required=False)\n",
    "#filepath = os.path.dirname(__file__)\n",
    "CLASSIFIER_DIR = os.path.join('/Users/iman/code/event_classifier/model/classifiers/')\n",
    "\n",
    "# Getting event text for classifier\n",
    "QUERY_ALL = \"\"\"\n",
    "\n",
    "SELECT event_id\n",
    ", p_class\n",
    ", s_class\n",
    ", t_class\n",
    ", event_name as event_name\n",
    ", event_type as event_type\n",
    ", event_host as event_host\n",
    ", event_subject as event_subject\n",
    ", text_paper as event_text\n",
    ", created\n",
    "FROM (\n",
    "    SELECT ce.event_id\n",
    "    , CASE WHEN ce.p_class = 'skip' THEN 'other' ELSE ce.p_class END\n",
    "    , CASE WHEN ce.s_class = 'skip' THEN 'other' ELSE ce.s_class END\n",
    "    , CASE WHEN ce.t_class = 'skip' THEN 'other' ELSE ce.t_class END\n",
    "    , e.name as event_name\n",
    "    , e.type as event_type\n",
    "    , e.host as event_host\n",
    "    , e.subject as event_subject\n",
    "    , listagg(TRIM(lower(cat.text))) as text_paper\n",
    "    , ce.created_at as created\n",
    "    FROM event_training_selections ce\n",
    "    JOIN events e ON e.id = ce.event_id\n",
    "    JOIN cards c ON c.event_id = ce.event_id\n",
    "    JOIN card_sides cs ON cs.card_id = c.id\n",
    "        AND cs.side_type_id = 0\n",
    "    LEFT JOIN card_assets cat ON cat.card_side_id = cs.id\n",
    "        AND cat.asset_type_id = 9\n",
    "    WHERE ce.is_confirmed\n",
    "    GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 10\n",
    "    )\n",
    "WHERE len(trim(event_name || ' ' || event_host || ' ' || event_subject ||\n",
    "        ' ' || text_paper))\n",
    "ORDER BY random()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "QUERY_EVENTS = \"\"\"\n",
    "\n",
    "SELECT event_id\n",
    ", event_name as event_name\n",
    ", event_type as event_type\n",
    ", event_host as event_host\n",
    ", event_subject as event_subject\n",
    ", text_paper as event_text\n",
    ", created\n",
    "FROM (\n",
    "    SELECT e.id as event_id\n",
    "    , e.name as event_name\n",
    "    , e.type as event_type\n",
    "    , e.host as event_host\n",
    "    , e.subject as event_subject\n",
    "    , listagg(TRIM(lower(cat.text))) as text_paper\n",
    "    , e.created_at as created\n",
    "    FROM events e\n",
    "    JOIN cards c ON c.event_id = e.id\n",
    "    JOIN card_sides cs ON cs.card_id = c.id\n",
    "        AND cs.side_type_id = 0\n",
    "    LEFT JOIN card_assets cat ON cat.card_side_id = cs.id\n",
    "        AND cat.asset_type_id = 9\n",
    "    WHERE e.id LIKE {0}\n",
    "    GROUP BY 1, 2, 3, 4, 5, 7\n",
    "    )\n",
    "WHERE len(trim(event_name || ' ' || event_host || ' ' || event_subject ||\n",
    "        ' ' || text_paper))\n",
    "ORDER BY random()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    \"\"\"\n",
    "    Using Counter find most common element in list\n",
    "    Possible results:\n",
    "        mc = [(val1,3)]\n",
    "        mc = [(val1,2),(val2,1)]\n",
    "        mc = [(val1,1),(val2,1),(val3,1)]\n",
    "        ...\n",
    "    No majority exists only when there is a tie, otherwise the first value of\n",
    "    the list is the most common (because Counter sorts automatically in\n",
    "    descending order by value)\n",
    "    \"\"\"\n",
    "\n",
    "    # check if top two most common predictions are the same\n",
    "    mc = Counter(lst).most_common(2)\n",
    "\n",
    "    if len(mc) > 1 and mc[0][1] == mc[1][1]:\n",
    "        return \"no_majority\"\n",
    "\n",
    "    return mc[0][0]\n",
    "\n",
    "\n",
    "def get_ensemble_prediction(results, classes):\n",
    "    \"\"\"\n",
    "    if majority of classifiers choose same category, that's the winner.\n",
    "    if majority does not exist, then select class with highest probability\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting ensemble predictions...\")\n",
    "    num_clfs = len(results)\n",
    "\n",
    "    # combine all classifier predictions and probabilities\n",
    "    all_preds = np.array([v[0] for k, v in results.items()]).T\n",
    "    all_probs = np.sum(np.array([v[1] for k, v in results.items()]), axis=0)\n",
    "    all_probs_normalize = all_probs/num_clfs\n",
    "\n",
    "    # the function most_common returns majority class or \"no_majority\"\n",
    "    majority = np.array(list(map(most_common, all_preds)))\n",
    "    no_majority_index = np.where(majority == 'no_majority')\n",
    "\n",
    "    # for those where a majority doesn't exist, sum the probabilities for each\n",
    "    # class\n",
    "    no_majority_sum = all_probs_normalize[no_majority_index]\n",
    "\n",
    "    # ensure no new probabilities were added that shouldn't be\n",
    "    assert np.allclose(np.sum(no_majority_sum), len(no_majority_index[0])), (\n",
    "           \"probability sum is greater than expected for no_majority\")\n",
    "\n",
    "    # replace the \"no_majority\" samples with the class that resulted in the\n",
    "    # largest probability\n",
    "    majority[no_majority_index] = classes[np.argmax(no_majority_sum, axis=1)]\n",
    "\n",
    "    return majority, np.max(all_probs_normalize, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_classifier_list(clf):\n",
    "    \"\"\"\n",
    "    Load a classifier\n",
    "    Classifier is stored as list object\n",
    "    Returns list of classifiers and their names\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading classifier list...\")\n",
    "    # clean up input: remove \"classifier/\" and \".pkl\"\n",
    "    clf_id = clf[clf.find('/')+1:-4]\n",
    "\n",
    "    clf_list, clf_names = [], []\n",
    "    # this is a list of classifiers\n",
    "    loaded_clf = load_classifier(CLASSIFIER_DIR + clf_id)\n",
    "    for classifier in loaded_clf:\n",
    "        clf_list.append(classifier)\n",
    "\n",
    "        # get name via class structure\n",
    "        clf_class = str(classifier.named_steps['clf'].__class__)\n",
    "\n",
    "        # some basic cleaning of class name\n",
    "        clf_name_indx = clf_class.find('.')\n",
    "        clf_name = clf_class[clf_name_indx+1:-2]\n",
    "        clf_names.append(clf_name)\n",
    "\n",
    "    return clf_list, clf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_event_types(df):\n",
    "    \n",
    "    acceptable_event_types = np.array([\n",
    "        'BasicAnnouncement', 'DatedAnnouncement', 'GreetingCard', 'LinkAway', 'RsvpEvent'])\n",
    "    \n",
    "    for i in df.event_type.values:\n",
    "        assert (i in acceptable_event_types) == True, \"Found event type that doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_primary_classes(y):\n",
    "    \n",
    "    acceptable_primary_classes = np.array([\n",
    "        'birthday_celebration', 'greetings', 'organizations', 'other',\n",
    "       'personal', 'wedding_related'])\n",
    "    \n",
    "    for i in y.primary.values:\n",
    "        assert (i in acceptable_primary_classes) == True, \"Found event type that doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_null(df):\n",
    "    \"\"\"\n",
    "    Ensure no NULL values\n",
    "    \"\"\"\n",
    "    \n",
    "    assert np.sum(pd.isnull(df).values) == 0, \"Some NULL values exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    \"\"\"\n",
    "    Perform any necessary cleaning\n",
    "    \n",
    "    1) Remove other class types\n",
    "    2) ensure no NULL values\n",
    "    3) ensure event_types are corrects\n",
    "    4) ensure primary classes are correct\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df[~(df.s_class == 'other')].copy()\n",
    "    \n",
    "    \n",
    "    check_null(df)\n",
    "    check_event_types(df)\n",
    "    check_primary_classes(y)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_and_y(df):\n",
    "    \"\"\"\n",
    "    Input: dataframe based on query\n",
    "    Output: X and y (type dataframe)\n",
    "    \"\"\"\n",
    "\n",
    "    df = clean_df(df)\n",
    "    \n",
    "    X = pd.DataFrame([df.event_subject, df.event_text, df.event_type]).T\n",
    "    X.columns = ['subject', 'text', 'event_type']\n",
    "\n",
    "    y = pd.DataFrame([df.p_class, df.s_class, df.t_class]).T\n",
    "    y.columns = ['primary', 'secondary', 'tertiary']\n",
    "\n",
    "    assert X.shape[0] == y.shape[0], 'X and y must be of same dimension'\n",
    "\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_primary_training_features(X, y, FEATURE_PIPELINE):\n",
    "    \"\"\"\n",
    "    Combines training features\n",
    "    Using fit_transform on the pipeline object for each feature\n",
    "    Performs sparse matrix concatenation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Beginning pipeline fit_transform to training data...\")\n",
    "    \n",
    "    subject_matrix = FEATURE_PIPELINE['subject_pipe'].fit_transform(X.subject, y)\n",
    "    text_matrix = FEATURE_PIPELINE['text_pipe'].fit_transform(X.text, y)\n",
    "    event_type_matrix = FEATURE_PIPELINE['event_type_pipe'].fit_transform(X.event_type, y)\n",
    "    \n",
    "    X = hstack([subject_matrix, text_matrix, event_type_matrix])\n",
    "    \n",
    "    print(\"Completed fit_transform\")\n",
    "    \n",
    "    check_dimensions(X, subject_matrix, text_matrix, event_type_matrix)\n",
    "    print(\"Training set dimension:\", X.shape)\n",
    "    \n",
    "    return X, pipes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_features(feature_pipeline, level):\n",
    "    \"\"\"\n",
    "    Save training features\n",
    "    Need to create an id_num to know which classifier to load - must be the same as the feature id number\n",
    "    \"\"\"\n",
    "    \n",
    "    id_num = np.random.randint(1000)\n",
    "    logger.info(\"Saving fitted feature pipeline id {}...\".format(id_num))\n",
    "    pickle_classifier(feature_pipeline, CLASSIFIER_DIR + level + \"_pipeline_\" + str(id_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_classifier(clf, level, id_num):\n",
    "    \"\"\"\n",
    "    Save trained classifier\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Saving trained classifier id {}...\".format(id_num))\n",
    "    pickle_classifier(clf, CLASSIFIER_DIR + level + \"_classifier_\" + str(id_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters for gridsearch\n",
    "# using SVM currently\n",
    "CLASSIFIER_PIPELINE = dict({\n",
    "    'SVM': {\n",
    "        'classifier': Pipeline([\n",
    "            (\"clf\", SVC(probability=True)),\n",
    "        ]),\n",
    "        'params': {\n",
    "            'clf__C': [1],\n",
    "            'clf__kernel': ['linear']\n",
    "        }\n",
    "     },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_dimensions(X, subject_matrix, text_matrix, event_type_matrix, predicted_primary_matrix):\n",
    "    \n",
    "    assert (x_train_matrix.shape[1] == subject_matrix.shape[1] + text_matrix.shape[1] + \\\n",
    "            event_type_matrix.shape[1] + predicted_primary_matrix.shape[1]), \\\n",
    "    (\"Number of x_train features doesn't match sum of component features\")\n",
    "    \n",
    "    assert (x_train_matrix.shape[0] == subject_matrix.shape[0]),\\\n",
    "    (\"Number of x_train samples doesn't match subject_matrix samples\")\n",
    "\n",
    "    assert (x_train_matrix.shape[0] == text_matrix.shape[0]), \\\n",
    "    (\"Number of x_train samples doesn't match body text samples\")\n",
    "    \n",
    "    assert (x_train_matrix.shape[0] == event_type_matrix.shape[0]),\\\n",
    "    (\"Number of x_train samples doesn't match event_type_matrix samples\")\n",
    "\n",
    "    assert (x_train_matrix.shape[0] == predicted_primary_matrix.shape[0]),\\\n",
    "    (\"Number of x_train samples doesn't match event_type_matrix samples\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(X, y, gridsearch_pipeline):\n",
    "    \"\"\"\n",
    "    Perform a Grid Search over the space of classifiers and their associated\n",
    "    parameter space\n",
    "    Inputs: X and y training sets\n",
    "    Output: A list of the best classifiers from each classifier category\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"starting Gridsearch...\")\n",
    "\n",
    "    best_classifiers = []\n",
    "    names = []\n",
    "\n",
    "    for v in gridsearch_pipeline.items():\n",
    "        gs = GridSearchCV(v[1]['classifier'], v[1]['params'], verbose=2, cv=3, n_jobs=4)\n",
    "        gs = gs.fit(X, y)\n",
    "        names.append(v[0])\n",
    "        logger.info(\"{} finished\".format(v[0]))\n",
    "        logger.info(\"Best scoring classifier: {}\".format(gs.best_score_))\n",
    "        best_classifiers.append(gs.best_estimator_)\n",
    "\n",
    "    return best_classifiers, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: (46082, 3)\n"
     ]
    }
   ],
   "source": [
    "#df = execute_query(QUERY_ALL, event_id=False)\n",
    "\n",
    "X, y = get_X_and_y(df)\n",
    "print(\"Size of dataset:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, idx1, idx2 = train_test_split(X, y, X.index, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-579-b35a1dfd0028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#x_train_matrix, feature_pipeline = get_primary_training_features(x_train, y_train, FEATURE_PIPELINE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#x_test_matrix = get_primary_testing_features(x_test, feature_pipeline)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mid_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'primary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "x_train_matrix, feature_pipeline = get_primary_training_features(x_train, y_train, FEATURE_PIPELINE)\n",
    "x_test_matrix = get_primary_testing_features(x_test, feature_pipeline)\n",
    "id_num = save_features(feature_pipeline, 'primary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_primary = y_test['primary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting classifier results...\n"
     ]
    }
   ],
   "source": [
    "results, classes = get_classifier_results(clf_list, clf_names, x_test_matrix, y_test_primary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving classifier: /Users/iman/code/event_classifier/model/classifiers/primary_classifier193...\n"
     ]
    }
   ],
   "source": [
    "save_classifier(clf_list, 'primary', 193)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ensemble predictions...\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_score = get_ensemble_prediction(results, classes)\n",
    "check_prediction_dimensions(y_test_primary, y_pred, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_prediction_dimensions(y_test, y_pred, y_score):\n",
    "    \"\"\"\n",
    "    Ensure that prediction dimensions are correct\n",
    "    \"\"\"\n",
    "    \n",
    "    assert y_pred.shape[0] ==  y_test.shape[0], (\"Ensure class prediction vector is same length as test set\")\n",
    "    assert y_score.shape[0] ==  y_test.shape[0], (\"Ensure score prediction vector is same length as test set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         classification  precision    recall\n",
      "0  birthday_celebration   0.978728  0.958557\n",
      "1             greetings   0.941595  0.974131\n",
      "2         organizations   0.816908  0.767748\n",
      "3                 other   0.791822  0.678344\n",
      "4              personal   0.833678  0.882754\n",
      "5       wedding_related   0.940847  0.869873\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the model\n",
    "logger.info(\"------------------------------------------------\")\n",
    "logger.info(\"Overall Accuracy Primary:{}\".format(accuracy_score(y_test_primary, y_pred)))\n",
    "print(precision_recall_matrix(y_test_primary, y_pred, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load data id=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train secondary model on id=1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATURE_PIPELINE = dict({\n",
    "    'subject_pipe':Pipeline([\n",
    "                    ('cleanText', CleanTextTransformer()),\n",
    "                    ('vectorizer', CountVectorizer(tokenizer=tokenize_text,ngram_range=(1, 1))),\n",
    "                    ('tfidf', TfidfTransformer())\n",
    "                    ]),\n",
    "    'text_pipe': Pipeline([\n",
    "                    ('cleanText', CleanTextTransformer()),\n",
    "                    ('vectorizer', CountVectorizer(tokenizer=tokenize_text,ngram_range=(1, 1))),\n",
    "                    ('tfidf', TfidfTransformer())\n",
    "                    ]),\n",
    "    'event_type_pipe': Pipeline([('vectorizer', CountVectorizer())])\n",
    "    'predicted_primary': Pipeline([('vectorizer', CountVectorizer())])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_secondary_training_features(X, y, FEATURE_PIPELINE):\n",
    "    \"\"\"\n",
    "    Combines training features\n",
    "    Using fit_transform on the pipeline object for each feature\n",
    "    Performs sparse matrix concatenation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Beginning pipeline fit_transform to training data...\")\n",
    "    \n",
    "    subject_matrix = FEATURE_PIPELINE['subject_pipe'].fit_transform(X.subject, y)\n",
    "    text_matrix = FEATURE_PIPELINE['text_pipe'].fit_transform(X.text, y)\n",
    "    event_type_matrix = FEATURE_PIPELINE['event_type_pipe'].fit_transform(X.event_type, y)\n",
    "    predicted_primary_matrix = FEATURE_PIPELINE['predicted_primary'].fit_transform(X.pred_primary_class, y)\n",
    "    \n",
    "    X = hstack([subject_matrix, text_matrix, event_type_matrix, predicted_primary_matrix])\n",
    "    \n",
    "    print(\"Completed fit_transform\")\n",
    "    \n",
    "    check_dimensions(X, subject_matrix, text_matrix, event_type_matrix, predicted_primary_matrix)\n",
    "    print(\"Training set dimension:\", X.shape)\n",
    "    \n",
    "    return X, pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_secondary_testing_features(X, FEATURE_PIPELINE):\n",
    "    \"\"\"\n",
    "    Combines training features\n",
    "    Key difference between this and training_features\n",
    "    is that pipeline is transforming x_test not fit_transforming\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Beginning transform of test set...\")\n",
    "    subject_matrix = FEATURE_PIPELINE['subject_pipe'].transform(X.subject)\n",
    "    text_matrix = FEATURE_PIPELINE['text_pipe'].transform(X.text)\n",
    "    event_type_matrix = FEATURE_PIPELINE['event_type_pipe'].transform(X.event_type)\n",
    "    predicted_primary_matrix = FEATURE_PIPELINE['predoicted_primary'].transform(X.pred_primary_class)\n",
    "    \n",
    "    X = hstack([subject_matrix, text_matrix, event_type_matrix, predicted_primary_matrix])\n",
    "    \n",
    "    print(\"Completed transform of test set.\")\n",
    "    \n",
    "    check_dimensions(X, subject_matrix, text_matrix, event_type_matrix, predicted_primary_matrix)\n",
    "    print(\"Testing set dimension:\", X.shape)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pandas.io.pickle.read_pickle>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_pickle('956.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['true_secondary_class']\n",
    "X = df[['subject','text','event_type','pred_primary_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split training, testing\n",
    "x_train, x_test, y_train, y_test, idx1, idx2 = train_test_split(X, y, X.index, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_matrix, feature_pipeline = get_secondary_training_features(x_train, y_train, FEATURE_PIPELINE)\n",
    "x_test_matrix = get_secondary_testing_features(x_test, feature_pipeline)\n",
    "id_num = save_features(feature_pipeline, 'secondary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
